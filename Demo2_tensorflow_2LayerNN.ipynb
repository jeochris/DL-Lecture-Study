{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "Demo2_tensorflow_2LayerNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeochris/DL-Lecture-Study/blob/main/Demo2_tensorflow_2LayerNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJ1IkCVzsweh"
      },
      "source": [
        "# Example 2: Two layer neural network\n",
        "# 2.1 Loading the reuired libraries\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XtBAT_Fswem",
        "outputId": "8628edc8-62ee-4641-f112-9caa7d0de12e"
      },
      "source": [
        "!rm -rf /tmp/mylogs "
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'rm'은(는) 내부 또는 외부 명령, 실행할 수 있는 프로그램, 또는\n",
            "배치 파일이 아닙니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0Z5znSgswen"
      },
      "source": [
        "# 2.2 Defining the Batch (Sample) size, input dimension, hidden dimension and number of classes\n",
        "N,D,H,C=64,1000,100,10\n",
        "# Here, hidden layer=1, hidden nodes=100 => also tuning parameter"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHcgmfKGsweo"
      },
      "source": [
        "# 2.3 Defining the feedforward and loss function\n",
        "def forward(x):\n",
        "    a=tf.matmul(x,w1) # w1: input~hidden weight => linear combination\n",
        "    a_relu=tf.nn.relu(a) # relu function => hidden layer values\n",
        "    scores=tf.matmul(a_relu,w2) # w2: hidden~output weight\n",
        "    return scores\n",
        "\n",
        "def loss_fn(scores, y):\n",
        "    probs= tf.nn.softmax(scores) # softmax function => output (probability of classes)\n",
        "    loss= -tf.reduce_sum(y*tf.math.log(probs)) # y: real value ={0,1}\n",
        "    return loss    \n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FpGKVuh2swep",
        "outputId": "3509c481-8ed7-40e0-9668-3917ad3a8851"
      },
      "source": [
        "# 2.4 Simulating data matrices = Generate input and output data set\n",
        "xx=np.random.randn(N,D).astype(np.float32) # dimension D인 N개 input\n",
        "yy=np.zeros((N,C)).astype(np.float32) # 그거에 대한 output N개 (dimension C) => 10 dimension인 64개 결과\n",
        "yy[np.arange(N),np.random.randint(C,size=N)]=1 # output 10개 중에 하나를 1로 (classification)\n",
        "yy\n",
        "xx\n",
        "print(xx)\n",
        "print(yy)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-0.56598413  0.23953073  0.01121933 ...  0.18119116  1.3432348\n",
            "  -0.8247276 ]\n",
            " [-0.9243525   0.24899566 -1.0412728  ...  1.6390234  -1.617978\n",
            "   0.32010147]\n",
            " [-0.520543    0.5141282   0.07274141 ... -0.16087748  0.94700056\n",
            "  -0.26457566]\n",
            " ...\n",
            " [ 0.23558384 -0.00584414  0.01846702 ... -0.6080901  -1.0966027\n",
            "  -1.0457603 ]\n",
            " [-0.42947787 -0.7071345   2.4097633  ... -0.726892   -0.04989825\n",
            "  -0.42902738]\n",
            " [-0.5078557  -1.2014279  -0.5000859  ... -0.91358835  0.96519935\n",
            "   0.4983903 ]]\n",
            "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B08fX1mMswep"
      },
      "source": [
        "# 2.5 Defining the Variables; Weight matrices w1 and w2 (initial weight parameter - to be updated)\n",
        "w1=tf.Variable(1e-3*np.random.randn(D,H).astype(np.float32),name=\"w1\") # input~hidden => D*H개 weight 필요\n",
        "w2=tf.Variable(1e-3*np.random.randn(H,C).astype(np.float32),name=\"w2\") # hidden~output => H*C개 weight 필요"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YETCKR-wsweq"
      },
      "source": [
        "# 2.7 Perform training (Forward, Back)\n",
        "learning_rate=1e-2 # tuning parameter\n",
        "optimizer=tf.keras.optimizers.Adam(learning_rate)\n",
        "\n",
        "def train_step(x, y):\n",
        "    with tf.GradientTape() as tape:\n",
        "        scores = forward(x)\n",
        "        loss = loss_fn(scores, y) # softmax -> loss function\n",
        "    gradients = tape.gradient(loss, [w1, w2]) # calc gradients\n",
        "    optimizer.apply_gradients(zip(gradients, [w1, w2])) # apply gradients to update weight\n",
        "    return loss, w1, w2\n",
        "    \n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAStbG71xJud"
      },
      "source": [
        "**Should try different tuning parameters**\n",
        "* learning rate\n",
        "* hidden layer number\n",
        "* hidden nodes number\n",
        "\n",
        "=> important when fitting real data sets (project too)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1E6XWEvyswer"
      },
      "source": [
        "# 2.8 Creating logs for Tensorboard\n",
        "summary_writer = tf.summary.create_file_writer('/tmp/mylogs')\n",
        "loss_list = []\n",
        "\n",
        "for step in range(10): # iterate 10 times\n",
        "    loss, w1, w2 = train_step(xx, yy)\n",
        "    loss_list.append(loss.numpy())\n",
        "    \n",
        "    with summary_writer.as_default():\n",
        "        tf.summary.scalar('loss_summary', loss, step=step)\n",
        "        tf.summary.histogram('w1_hist', w1, step=step)\n",
        "        tf.summary.histogram('w2_hist', w2, step=step)\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W3WHhXEyswes",
        "outputId": "dd86f484-bb28-4385-f392-a392ad0ec5e6"
      },
      "source": [
        "# 2.9 Check loss => loss (=error) decreases after iterating\n",
        "loss_list"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[147.36679,\n",
              " 141.02768,\n",
              " 95.74078,\n",
              " 40.37176,\n",
              " 9.652254,\n",
              " 1.5853906,\n",
              " 0.13175254,\n",
              " 0.0080469735,\n",
              " 0.00046636222,\n",
              " 2.7775823e-05]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkmsLUmqswet"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}